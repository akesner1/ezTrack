{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "The following code was designed in order to track the location of a single animal across the course of a single video session.  After initally loading in the video, the user is able to crop the video frame using a rectangle selection tool.  A background reference frame is then specified, either by taking a median of several frames in the video, or by the user providing a short video of the same environment without an animal in it.  By comparing each frame in the video to the reference frame, the location of the animal can be tracked.  It is imperative that the reference frame of the video is not shifted from the actual video.  For this reason, if a separate video is supplied, it is best that it be acquired on the same day as the behavioral recording.  The animal's center of mass, in x,y coordinates, is then recorded, as well as the distance in pixels that the animal moves from one frame to the next.  Optionally, the orientation of the animal can also be tracked. Lastly, the user can specify regions of interest in the frame (e.g. left, right) using a polygon drawing tool and record for each frame whether the animal is in the region of interest.  Moreover, if the user would like to measure interactions with stationary objects, the locations of objects can be given (this requires that the orientation of the animal is tracked).  Options for summarizing the data are also provided. \n",
    "\n",
    "### Package Requirements\n",
    "Please see instructions under repository README for package requirements and install instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Load Necessary Packages\n",
    "The following code loads neccessary packages and need not be changed by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import LocationTracking_Functions as lt\n",
    "import holoviews as hv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. User Sets Directory and File Information, and Specifies ROI Names, if any.\n",
    "Below the user sets the directory file path (path of the folder where the video file is), the file name of the video to be analyzed, the frame rate of the video, the frame on which the analysis is to begin (0 is the first frame), and the last frame to be analyzed (set `'end' : None` if processing entire video).  Additionally, the user defines the names of any regions of interest as well as the names of any objects of interest.  Any number of regions of interest/objects can be used.  If no regions of interest are to be used, set `region_names = None`. Similarly, if the interaction with stationary objects is not of interest, set `object_names = None`. By modifying 'stretch', one can alter the relative width/height of the presented output.\n",
    "\n",
    "***Windows Users:*** Place an 'r' in front directory path (e.g. r\"zp\\Videos\") to avoid mishandling of forward slashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict = {\n",
    "    'dpath' : \"/Users/me/myVideos\",    \n",
    "    'file' : 'Video.avi',\n",
    "    'fps' : 30, \n",
    "    'start' : 0, \n",
    "    'end' : None\n",
    "}\n",
    "stretch = dict(width = 1, height = 1)\n",
    "\n",
    "region_names = None#['Left','Right']\n",
    "object_names = None#['Object1','Object2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Load Video and Crop Frame if Desired\n",
    "To crop video frame, after running code below, select box selection tool below image (square with a plus sign).  To start drawing region to be included in analyis, double click image.  Double click again to finalize region.  If you decide to change region, it is best to rerun this cell and subsequent steps.\n",
    "\n",
    "If the size of the image is too small/large, alter the first line of code.  100 is the standard size.  200 will produce an image 2x the size, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "image,crop,video_dict = lt.LoadAndCrop(video_dict,stretch,cropmethod='Box')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Define Reference Frame for Location Tracking.\n",
    "For location tracking to work, view of box without animal must be provided.  Below there are two ways to do this.  **Option 1** provides a method to remove the animal from the video.  This option works well provided the animal doesn't stay in the same location for >50% of the session. Alternatively, with **Option 2**, the user can simply define a video file in the same folder that doesn't have in animal in it.  Option 1 is generally recormmended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Create reference frame by removing animal from video\n",
    "The following code takes 100 random frames across the session and creates a reference image by taking the median for each pixel.  This will remove influence of animal on any given frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "reference, image = lt.Reference(video_dict,stretch,crop=crop,num_frames=100) \n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - User specifies video of empty box\n",
    "The following code allows the user to specify a different file.  Set video_dict['altfile'] to the alternative filename.  Notably, a median is still taken of multiple frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "video_dict['altfile'] = 'EmptyBox.avi' \n",
    "reference, image = lt.Reference(video_dict,stretch,crop=crop,num_frames=100,altfile=True) \n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. (Optional) Use Interactive Plot to Define Regions of Interest and Objects of Interest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Draw Regions of Interest\n",
    "After running cell below, draw regions of interest on presented image in the order you provided them.  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell.\n",
    "\n",
    "***Note*** that there are no problems if regions overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "plot,roi_stream = lt.ROI_plot(reference,region_names,stretch)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Draw Objects of Interest\n",
    "After running cell below, draw objects of interest on presented image in the order you provided them.  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "plot,object_stream = lt.ROI_plot(reference,object_names,stretch)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. (Optional) Define Scale for Distance Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Select two points of known distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running cell below, click on any two points and the distance between them, in pixel units, will be presented/returned. Will be used to convert pixel distance to other scale. Note that once drawn, points can be dragged or you can click again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 100\n",
    "\n",
    "dist_plot, dist = lt.DistanceTool(reference,stretch)\n",
    "dist_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Define real-world distance between points\n",
    "Below, set the distance between the points selected above, and the scale. Note that scale can be any desired text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = {\n",
    "    'distance' : 100,\n",
    "    'scale' : 'cm' \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Track Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Set Location Tracking Parameters\n",
    "Location tracking examines the deviance of each frame in a video from the reference frame on a pixel by pixel basis.  For each frame it calculates the center of mass for these differences (COM) to define the center of the animal.  \n",
    "\n",
    "In order to improve accuracy, the parameter loc_thresh is used to remove the influence of pixels that are minimally different from the reference frame.  For each frame relative to the reference frame, the distribution of absolute difference values across pixels is examined and only values above loc_thresh percentile are considered.  We have been using 99-99.5 and this works well.  Values can range from 0-100 and floating point numbers are accepted.\n",
    "\n",
    "The parameters: use_window, window_size, and window_weight are employed to reduce the chance that any objects other than the animal  that might enter the frame (e.g. the hand of the experimenter) influence location tracking.  For each frame, a square window with the animal's position on the prior frame at its center is given more weight when searching for it's location (because an animal presumably can't move far in a fraction of a second).  When window_weight is set to 1, pixels outside of the window are not considered at all; at 0, they are given equal weight.  Notably, setting a high value that is still not equal to 1 (e.g. 0.9) should allow the program to more rapidly find the animal if by chance it moves out of the window.\n",
    "\n",
    "The paramater method accepts values of 'abs', 'light', and 'dark'. 'abs' does not take into consideration whether the animal is lighter or darker than the background and will therefore track the animal across a wide range of backgrounds. 'light' assumes the animal is lighter than the background, and 'dark' assume the animal is darker than the background. 'abs' generally works well, but there are situations in which you may wish to use the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_params = {\n",
    "    'loc_thresh' : 99, \n",
    "    'use_window' : True, \n",
    "    'window_size' : 100, \n",
    "    'window_weight' : .9, \n",
    "    'method' : 'abs',\n",
    "    'angle_params' : None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. (Optional) Set parameters for tracking orientation\n",
    "Below, parameters are set for tracking an animal's orientation and its interactions with objects. A triangular search arc will extend from the center of mass, and on any given frame, if an object of interest is contained within this arc the animal will be defined as interacting with it (i.e. an animal is interacting with an object if it is facing it and within a user specified distance of the object). `angle_length` defines the length the arc extends from the animal. `angle_arc` defines the width of the arc, in degrees.  `angle_mindist` defines the minimum distance the animal must move for the arc to be updated (necessary to ignore small movements when the animal is stationary). This could reasonably be set to the animal's median movement across all frames (print when 7d is run), but if the animal's orientation fails to follow the animal (see step 9 for visualization), try setting this lower. Alternatively, if it flips back and forth rapidly, try setting this higher. Lastly, `angle_start`, sets the angle on the first frame, though this typically is not necessary and can be left at 0.  Because the algorithm uses motion to calculate angle, if the animal is motionless at the beginning of the video, the angle can start out wrong.  By aproximating the angle, in degrees (0-360), this can be alleviated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_params = {\n",
    "    'angle_length' : 30,\n",
    "    'angle_arc' : 60,\n",
    "    'angle_mindist' : 1,\n",
    "    'angle_start' : 0\n",
    "}\n",
    "tracking_params['angle_params'] = angle_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c. Display Examples of Location Tracking to Confirm Threshold\n",
    "In order to confirm threshold is working, a subset of images is analyzed and displayed using the selected tracking_params.  The original image is displayed on top and the difference values are presented below.  The center of mass (COM) is pinpointed on images.  Notably, because individual frames are used, window settings are not applicable here.  Because of this, actual tracking in video is likely to be better.\n",
    "\n",
    "The user can change the number examples below as they see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "%%time\n",
    "\n",
    "examples = 5\n",
    "images=lt.LocationThresh_View(video_dict,reference,tracking_params,crop=crop,examples=examples,stretch=stretch)\n",
    "images.cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7d. Track Location and Save Results to .csv File\n",
    "For each frame the location of the animal's center of mass is recorded in x/y coordinates.  If ROIs are supplied, for each frame it is determined whether the animal is in each of the ROIs.  Frame-by-frame distance is also calculated in pixel units.  This data is returned in a Pandas dataframe with columns: frame, x, y, dist, and whether the animal is in each ROI specified (True/False).  Data is saved to a .csv in the same folder as the video.  First 5 rows of data are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=lt.TrackLocation(video_dict,tracking_params,reference,crop=crop)\n",
    "if object_names != None:\n",
    "    location = lt.Object_Interaction(reference,location,object_names,object_stream) \n",
    "if region_names != None: \n",
    "    location = lt.ROI_Location(reference,location,region_names,roi_stream) \n",
    "if 'scale_dict' in locals():\n",
    "    location = lt.ScaleDistance(scale_dict, dist, df=location, column='Distance_px')\n",
    "    \n",
    "location.to_csv(os.path.splitext(video_dict['fpath'])[0] + '_LocationOutput.csv')\n",
    "location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7e. Display Animal Distance/Location Across Session\n",
    "Below, the animals distance and location across the video is plotted.  Smooth traces are expected in the case where the animal is tracked consistently across the session.  Under heatmap, sigma controls 'binning' of location. When 'sigma = None' default value is provided; but sigma can also be set to any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "#Plot Distance Across Session\n",
    "w, h = 600,200 #specify width and height of plot\n",
    "dist_plot = hv.Curve((location['Frame'],location['Distance_px']),'Frame','Pixel Distance').opts(\n",
    "    height=h,width=w,color='red',title=\"Distance Across Session\",toolbar=\"below\")\n",
    "\n",
    "#Plot Trace of Animal Across Session and Generate Heatmap\n",
    "tracks = lt.showtrace(reference,location,color=\"red\",alpha=.5,size=2,stretch=stretch)\n",
    "heatmap = lt.Heatmap(reference, location, sigma=None,stretch=stretch)\n",
    "layout = (tracks+heatmap+dist_plot).cols(1)\n",
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. (Optional) Create Binned Summary Report and Save\n",
    "The code below allows the user to either save a csv containing summary data for user-defined bins (e.g. proportion of time in each region, proportion of time interacting with each object, and distance travelled for each minute) or a session-wide average. \n",
    "\n",
    "***If you only want a session avg***, set `bin_dict = None`\n",
    "\n",
    "To specify bins, set bin_dict using the following notation, where start and stop represent time ***in frames***:\n",
    "\n",
    "```\n",
    "bin_dict = {\n",
    "    'BinName1': (start, stop),\n",
    "    'BinName2': (start, stop),\n",
    "    'BinName3': (start, stop),\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Wanna get fancy?  Check out how to use Python dictionary comprehensions to make 20, 30 second bins, using 1 line of code:\n",
    "\n",
    "`bin_dict = {x:(x*30*60,(x+1)*30*60) for x in range(20)}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_dict = {\n",
    "    '1':(0,500),\n",
    "    '2':(500,1000),\n",
    "    '3':(1000,1500)\n",
    "}\n",
    "\n",
    "summary = lt.Summarize_Location(location,video_dict,bin_dict=bin_dict,region_names=region_names,object_names=object_names)\n",
    "if 'scale_dict' in locals():\n",
    "    summary = lt.ScaleDistance(scale_dict, dist, df=summary, column='Distance_px')\n",
    "summary.to_csv(os.path.splitext(video_dict['fpath'])[0] + '_SummaryStats.csv')\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. (Optional) View Video of Tracking\n",
    "**Note** that tracking must be done before this (Step 7c). \n",
    "The code below allows the user to play back a section of the analyzed video, with the center of mass of the animal marked, to confirm tracking.  User defines the start and end frames of the section they would like to watch.  Of note, if the user set the start frame for location tracking to something other than 0 in Step 2, the frame start and end here will be relative to that. Video can be saved by setting `'save_video' : True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dict = {\n",
    "    'start' : 0, \n",
    "    'stop' : 1000, \n",
    "    'save_video' : False \n",
    "}\n",
    "\n",
    "lt.PlayVideo(video_dict,display_dict,location,crop=crop)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
